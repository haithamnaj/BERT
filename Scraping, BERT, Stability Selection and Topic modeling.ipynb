{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Graduation project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drxl4GsoiF14"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fy9gkEaUFoS"
      },
      "source": [
        "!pip install pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfy78v5hEWpm"
      },
      "source": [
        "!python -m pip install -U pyLDAvis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2HLrphLKiLG"
      },
      "source": [
        "!pip install scikit-learn==0.19.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-KC1_DkGtCC"
      },
      "source": [
        "!pip install language_tool_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zx0mxOjiLBB"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMUGjXaxiXbc"
      },
      "source": [
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr0R-QtIiZQz"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4e7kAs8kOrR"
      },
      "source": [
        "pip install transformers==3.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3WIBNnKiZTY"
      },
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be0Ne-2EiZWG"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ibp-ouiZZb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "# import transformers as ppb\n",
        "import re, nltk, spacy, string\n",
        "import pandas as pd \n",
        "# from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from pprint import pprint\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from plotly.offline import plot\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "#import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37wga4zRhp8n"
      },
      "source": [
        "## Scrape Amazon Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNMXV8UchUr1"
      },
      "source": [
        "# %%time\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# import pandas as pd\n",
        "\n",
        "# reviewlist = []\n",
        "\n",
        "# def get_soup(url):\n",
        "#     r = requests.get('http://localhost:8050/render.html', params={'url': url, 'wait': 2})\n",
        "#     soup = BeautifulSoup(r.text, 'html.parser')\n",
        "#     return soup\n",
        "\n",
        "\n",
        "# def get_reviews(soup):\n",
        "#     reviews = soup.find_all('div', {'data-hook': 'review'})\n",
        "#     try:\n",
        "#         for item in reviews:\n",
        "#             review = {\n",
        "#             'product': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
        "#             'title': item.find('a', {'data-hook': 'review-title'}).text.strip(),\n",
        "#             'rating':  float(item.find('i', {'data-hook': 'review-star-rating'}).text.replace('out of 5 stars', '').strip()),\n",
        "#             'body': item.find('span', {'data-hook': 'review-body'}).text.strip(),\n",
        "#             }\n",
        "#             reviewlist.append(review)\n",
        "#     except:\n",
        "#         pass\n",
        "\n",
        "# for x in range(1,500):\n",
        "#     soup = get_soup(f'https://www.amazon.com/Under-Armour-Charged-Assert-Running/product-reviews/B08DXCSW56/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews{x}')\n",
        "#     print(f'Getting page: {x}')\n",
        "#     get_reviews(soup)\n",
        "#     print(len(reviewlist))\n",
        "#     if not soup.find('li', {'class': 'a-disabled a-last'}):\n",
        "#         pass\n",
        "#     else:\n",
        "#         break\n",
        "\n",
        "\n",
        "# print('Fin.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrWQHvFRh1EI"
      },
      "source": [
        "\n",
        "# df = pd.DataFrame(reviewlist)\n",
        "# len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWY8kAQ5h3PP"
      },
      "source": [
        "# df.to_excel('Shoes2.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og0LSQueivpn"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnNRq2AJiw7g"
      },
      "source": [
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neULNejoi1ie"
      },
      "source": [
        "df = pd.read_csv(\"reviews.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEBJbukli1k2"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6wRnMX1i1nz"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skhLvFj_i1rQ"
      },
      "source": [
        "sns.countplot(df.score)\n",
        "plt.xlabel('review score');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pEatT8Djcm3"
      },
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating <= 2:\n",
        "    return 0\n",
        "  elif rating == 3:\n",
        "    return 1\n",
        "  else: \n",
        "    return 2\n",
        "\n",
        "df['sentiment'] = df.score.apply(to_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y23SZVyUjdJh"
      },
      "source": [
        "class_names = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbel3YuijdM6"
      },
      "source": [
        "ax = sns.countplot(df.sentiment)\n",
        "plt.xlabel('review sentiment')\n",
        "ax.set_xticklabels(class_names);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWgJnGkWjlkM"
      },
      "source": [
        "## Data Preprocessing\n",
        "- Add special tokens to separate sentences and do classification\n",
        "- Pass sequences of constant length (introduce padding)\n",
        "- Create array of 0s (pad token) and 1s (real token) called *attention mask*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5fEQ2EKlo7-"
      },
      "source": [
        "from transformers import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uUdyQDMjiXZ"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTq6SNuAjiay"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TlOPRCuj2pW"
      },
      "source": [
        "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894lFv_1j2tM"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Sentence: {sample_txt}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgsOQUI9l39S"
      },
      "source": [
        "### Special Tokens\n",
        "\n",
        "`[SEP]` - marker for ending of a sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP8qA3Rjj2wg"
      },
      "source": [
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO60XwVxl7r9"
      },
      "source": [
        "`[CLS]` - we must add this token to the start of each sentence, so BERT knows we're doing classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trRHmpHwl-z9"
      },
      "source": [
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1SRhgNPmPXA"
      },
      "source": [
        "## special token for padding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9a8JXa9mKPK"
      },
      "source": [
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vq6xyN-mcZM"
      },
      "source": [
        "## BERT understands tokens that were in the training set. Everything else can be \n",
        "\n",
        "---\n",
        "\n",
        "encoded using the `[UNK]` (unknown) token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rC4bHqmiDz"
      },
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV0IxgY3mmTB"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBiQ0ieXmp4-"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhLfQwNImp7T"
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9PU0UJrmp-b"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVy_Qo5Am0o_"
      },
      "source": [
        "## Choosing Sequence Length\n",
        "\n",
        "BERT works with fixed-length sequences. We'll use a simple strategy to choose the max length. Let's store the token length of each review:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TISIX3ZuiBi5"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df.content:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYDX4v0nm9xG"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9OpBgqunQW3"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "Most of the reviews seem to contain less than 128 tokens, but we'll be on the safe side and choose a maximum length of 160."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Y6Lz8GnOqF"
      },
      "source": [
        "MAX_LEN = 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVc_mRErnXOu"
      },
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO-Oj-AMnjq8"
      },
      "source": [
        "## Split the data into training, validation and testing\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swpv11s-owXL"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.content.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqANSj1-o0Uj"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYz4lYino2rj"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RB5ulk4o4Yr"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPycve4ApCGO"
      },
      "source": [
        "## Sentiment Analysis Using BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJlCWsbgo6YX"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFRr1b5qpEWG"
      },
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyHkHQpjpGEj"
      },
      "source": [
        "last_hidden_state.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYmZImcJpH0e"
      },
      "source": [
        "bert_model.config.hidden_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdKdTBzQpJlM"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pukGjhYVpNAV"
      },
      "source": [
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD9qELebpPSv"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcGfF8lspRLe"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS7jy8iIpf3e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0EYGvX6pTNU"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2kyHCQDplls"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGh-qLvqpnu0"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7hDlxQupqDK"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6D2RHGFpsW1"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pYMLNIpy4g"
      },
      "source": [
        "## Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoIj8PGypvp5"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhS6r21Dp11w"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHkccU8_p4MN"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMtvBhKzp9ia"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJpfN71Xp_ht"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTqS06SUqCE4"
      },
      "source": [
        "idx = 2\n",
        "\n",
        "review_text = y_review_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce4-LrR_qEnx"
      },
      "source": [
        "print(\"\\n\".join(wrap(review_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EenbRcZ1qGbv"
      },
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOXdQIv5sLLe"
      },
      "source": [
        "## Detect the fake reviews \n",
        "\n",
        "\n",
        "1.  Remove the reviews that have a 100% correct grammar \n",
        "2.  Remove the reviews that the title and the content is same\n",
        "3.  Remove the reviews if the number of verbs > number of nouns in the review \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTOpXkxsDjD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDqmR0zysDwK"
      },
      "source": [
        "#stability selection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWN4EhJjsU1T"
      },
      "source": [
        "df2 = pd.read_csv('/content/df.csv')  \n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBjbMMrESmAB"
      },
      "source": [
        "df2 = df2.dropna()\n",
        "df2 = df2.reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuHrTt5YsUV8"
      },
      "source": [
        "# from sklearn.linear_model import RandomizedLasso\n",
        "# randomized_lasso = RandomizedLasso() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onj6cWKZQjSN"
      },
      "source": [
        "# pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEv2X5n7Qo0y"
      },
      "source": [
        "# from sklearn.linear_model import RandomizedLasso\n",
        "# randomized_lasso = RandomizedLasso() \n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from matplotlib import pyplot as plt\n",
        "# import seaborn as sns\n",
        "# %matplotlib inline\n",
        "# from sklearn.feature_selection import RFE, f_regression\n",
        "# from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65nGooqbsUX7"
      },
      "source": [
        "# bert1 = list(df2['reviewText'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFyaGxbPsUak"
      },
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "# embeddings = model.encode(review_text, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYV2_177WGr-"
      },
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# X = scaler.fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnfTSTkRRUVv"
      },
      "source": [
        "# Y=  df2['sentiment'].array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSaqgW55WoUb"
      },
      "source": [
        "# from sklearn.linear_model import RandomizedLogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "# clf = RandomizedLogisticRegression(sample_fraction = 0.25,selection_threshold = 0.4, n_resampling = 200)\n",
        "# clf.fit(X,Y)\n",
        "# print (clf.get_support())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGb8EqfdWz8X"
      },
      "source": [
        "# a = (clf.get_support(indices=True))\n",
        "# print(len(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aebUkVmpLtsq"
      },
      "source": [
        "# X = pd.DataFrame(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyPdypYAKIaw"
      },
      "source": [
        "# a = a.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfPRWgw8ehPf"
      },
      "source": [
        "# New = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyoSFzpHHPnY"
      },
      "source": [
        "\n",
        "# iterator = 1\n",
        "# for i in a:\n",
        "#   if i in X.columns:\n",
        "#     col_name = 'col' + str(i)\n",
        "#     New.loc[:, col_name] = X[i] \n",
        "#     iterator += 1 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPVz9QH2fI97"
      },
      "source": [
        "# New"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZyiKcy8iE9X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnMJEDMcsV5p"
      },
      "source": [
        "# import language_tool_python\n",
        "# tool = language_tool_python.LanguageTool('en-US')\n",
        "# token = []\n",
        "# neglected = []\n",
        "# size = df2['body'].size\n",
        "# for i in range (len(df2['body'])):\n",
        "#     if  len(tool.check(df2['body'][i])) > 0  :\n",
        "#         token.append(i)\n",
        "#     else:\n",
        "#         neglected.append(i)\n",
        "\n",
        "# len(neglected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwYzRNzLsV8e"
      },
      "source": [
        "# for i in (neglected):\n",
        "#   df2 =  df2.drop(i)\n",
        "# df2 = df2.reset_index(drop=True)\n",
        "# len(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FE-ANy2tcVe"
      },
      "source": [
        "# #1. different sentiment in review headline and review body\n",
        "\n",
        "# remove_reviews = []\n",
        "# # stores the list of review_id of fake reviews\n",
        "\n",
        "# for i in range(len(df2['body'])):\n",
        "#     #iterate through the whole dataset\n",
        "    \n",
        "#         if(( df2[\"title\"][i] ) == ( df2[\"body\"][i] ) ):\n",
        "#           remove_reviews.append(i)\n",
        "#             # checking if the sentiment of the body and the headline are not same\n",
        "            \n",
        "#             # remove_reviews.append(dataset[\"review_id\"][i])\n",
        "#             # append review_id to the list of fake reviews."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVd7_afztcZA"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# remove_reviews = []\n",
        "# for i in range(len(df2['body'])):\n",
        "#     #iterate the whole dataset\n",
        "    \n",
        "#     words = nltk.word_tokenize(str(df2[\"body\"][i]))\n",
        "#     #storing the words from the reviews into the list\n",
        "    \n",
        "#     tagged_words = nltk.pos_tag(words)\n",
        "#     # returns list of tuples of words along with their parts of speech\n",
        "    \n",
        "#     nouns_count = 0\n",
        "#     verbs_count = 0\n",
        "    \n",
        "#     for j in range(len(tagged_words)):\n",
        "#         #iterate through all the words\n",
        "\n",
        "#         if(tagged_words[j][1].startswith(\"NN\")):\n",
        "#             nouns_count+=1\n",
        "#             #counts the no. of nouns in the review\n",
        "\n",
        "#         if(tagged_words[j][1].startswith(\"VB\")):\n",
        "#             verbs_count+=1\n",
        "#             #counts the no. of verbs in the review\n",
        "\n",
        "#     if(verbs_count>nouns_count):\n",
        "#         #comparing the no. of verbs and nouns\n",
        "#         remove_reviews.append(i)\n",
        "#         #storing the review to be removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKXQrj8RsV_l"
      },
      "source": [
        "# print(len(remove_reviews))\n",
        "# for i in(remove_reviews):\n",
        "#     df2= df2.drop(i)\n",
        "\n",
        "# df2 = df2.reset_index(drop=True)\n",
        "# print(len(df2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS9w5w1lrhcs"
      },
      "source": [
        "## Preprocessing the predictied data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSSpREjDrq2_"
      },
      "source": [
        "#df2[\"review\"] = df2[\"body\"].str.lower()\n",
        "df2[\"review\"] = df2[\"reviewText\"].str.lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jj_kSu_rqzg"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1QG85jr0F_"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "df2[\"review\"] = df2[\"review\"].apply(lambda text: remove_stopwords(text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfz7XXKor0JC"
      },
      "source": [
        "import string\n",
        "import re\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df2[\"review\"] = df2[\"review\"].apply(lambda text: remove_punctuation(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW8iYLryr8TI"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "df2[\"review\"] = df2[\"review\"].apply(lambda text: remove_emoji(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVhXXIN7r8gK"
      },
      "source": [
        "def remove_urls(text):\n",
        "  \n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    try:\n",
        "        return url_pattern.sub(r'', text)\n",
        "    except:\n",
        "        print(text)\n",
        "    \n",
        "df2[\"review\"] = df2[\"review\"].apply(lambda text: remove_urls(text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioTBMxC6rquZ"
      },
      "source": [
        "review_text = df2['reviewText'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Jv3wUUqTLa"
      },
      "source": [
        "## Predicting product reviews into positive, negative and neutral "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmDYrEDKqI1x"
      },
      "source": [
        "positive_reviews = []\n",
        "negative_reviews = []\n",
        "neutral_reviews = []\n",
        "\n",
        "for i in range(len(review_text)):\n",
        "  encoded_review = tokenizer.encode_plus(\n",
        "      review_text[i],\n",
        "      max_length=MAX_LEN,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',)\n",
        "  input_ids = encoded_review['input_ids'].to(device)\n",
        "  attention_mask = encoded_review['attention_mask'].to(device)\n",
        "  output = model(input_ids, attention_mask)\n",
        "  _, prediction = torch.max(output, dim=1)\n",
        "  if class_names[prediction] == \"positive\":\n",
        "    positive_reviews.append(df2['review'][i])\n",
        "  elif class_names[prediction] == \"negative\":\n",
        "    negative_reviews.append(df2['review'][i])\n",
        "  elif class_names[prediction] == \"neutral\":\n",
        "    neutral_reviews.append(df2['review'][i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_mQN1HXFPW5"
      },
      "source": [
        "print(len(positive_reviews))\n",
        "print(len(negative_reviews))\n",
        "print(len(neutral_reviews))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvl79JkfGOih"
      },
      "source": [
        "df_positive = pd.DataFrame(positive_reviews)\n",
        "df_negative = pd.DataFrame(negative_reviews)\n",
        "df_neutral = pd.DataFrame(neutral_reviews)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfQ7TgGD-ONq"
      },
      "source": [
        "df_positive1 = df_positive.to_csv(\"df_Positive.csv\")\n",
        "df_negative1 = df_negative.to_csv(\"df_Negative.csv\")\n",
        "df_neutral1 = df_neutral.to_csv(\"df_Neutral.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Pbiot6WIE3"
      },
      "source": [
        "# positive_reviews1 = []\n",
        "# negative_reviews2 = []\n",
        "# neutral_reviews3 = []\n",
        "\n",
        "# for i in range(0,50):\n",
        "#   encoded_review = tokenizer.encode_plus(\n",
        "#       review_text[i],\n",
        "#       max_length=MAX_LEN,\n",
        "#       add_special_tokens=True,\n",
        "#       return_token_type_ids=False,\n",
        "#       padding=True,\n",
        "#       return_attention_mask=True,\n",
        "#       return_tensors='pt',)\n",
        "#   input_ids = encoded_review['input_ids'].to(device)\n",
        "#   attention_mask = encoded_review['attention_mask'].to(device)\n",
        "#   output = model(input_ids, attention_mask)\n",
        "#   _, prediction = torch.max(output, dim=1)\n",
        "#   if class_names[prediction] == \"positive\":\n",
        "#     positive_reviews1.append(df2['review'][i])\n",
        "#   elif class_names[prediction] == \"negative\":\n",
        "#     negative_reviews2.append(df2['review'][i])\n",
        "#   elif class_names[prediction] == \"neutral\":\n",
        "#     neutral_reviews3.append(df2['review'][i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkXQFbS8T5sx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCot651T5wN"
      },
      "source": [
        "## Topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhO5W947bL9"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(df2['review']))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOgpN8OsUnCr"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "\n",
        "bigram = gensim.models.Phrases(data_words) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=65)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kCBxE0tUrLQ"
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKivVnltUz2J"
      },
      "source": [
        "# Remove Stop Words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "# data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSDKGMy-U9Pu"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K62Vk0cVEZ8"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Build LDA model\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=4, \n",
        "                                           random_state=42,\n",
        "                                           update_every=25,\n",
        "                                           chunksize=100,\n",
        "                                           passes=30, \n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HruWAqr3VJDQ"
      },
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzozDV4ZVOYQ"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT46B25dAVc7"
      },
      "source": [
        "print(type(lda_model.print_topics()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0XOhD8CzJRr"
      },
      "source": [
        "\n",
        "for i in range (4):\n",
        "  words  = (lda_model.print_topics()[i])\n",
        "  s = ','.join(str(v) for v in words)\n",
        "  result = ''.join([i for i in s if not i.isdigit()])\n",
        "  a= re.sub(\"[\\\"\\']\", \"\", result)\n",
        "  a = a.replace('+', '')\n",
        "  a = a.replace('.', '')\n",
        "  a = a.replace('*', '')\n",
        "  a = a.replace(',', '')\n",
        "  res = a.split() \n",
        "  if i == 0:\n",
        "    b0 = pd.DataFrame(res,columns=['topic1'])\n",
        "  elif i == 1:\n",
        "    b1 = pd.DataFrame(res,columns=['topic2']) \n",
        "  elif i == 2:\n",
        "    b2 = pd.DataFrame(res,columns=['topic3'])\n",
        "  else:\n",
        "    b3 = pd.DataFrame(res,columns=['topic4'])\n",
        "   \n",
        "features = b0.join(b1)\n",
        "features = features.join(b2)\n",
        "features = features.join(b3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyR6Bk5Q8FXz"
      },
      "source": [
        "features =  (pd.concat([features, features.unstack().reset_index(drop=True).rename('all features')], axis=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXhKoLYosJYn"
      },
      "source": [
        "all_aspects = features['all features']\n",
        "all_aspects = pd.DataFrame(all_aspects)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRycKTLvUHE9"
      },
      "source": [
        "x=[]\n",
        "for i in range(0,40):\n",
        "  x.append(i)\n",
        "print(x)\n",
        "\n",
        "dct = {}\n",
        "for i in x:\n",
        "  dct [i]= []\n",
        "\n",
        "print(dct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvpLjiOJH55C"
      },
      "source": [
        "for i in range(0,40):\n",
        "  for j in range (0, len(df2['review'])):\n",
        "    if features['all features'][i] in df2['review'][j]:\n",
        "      dct[i].append(df2['review'][j])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEiyfbgcX56J"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('movie_reviews')\n",
        "from textblob import TextBlob\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq7QHc61C7Q1"
      },
      "source": [
        "y=[]\n",
        "for i in range(0,40):\n",
        "  y.append(i)\n",
        "print(y)\n",
        "\n",
        "dcti = {}\n",
        "for i in y:\n",
        "  dcti [i]= []\n",
        "\n",
        "print(dcti)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3ApfVR-Vxgi"
      },
      "source": [
        "aspect0 = pd.DataFrame(dct[0], columns=[('feature')])\n",
        "aspect1 = pd.DataFrame(dct[1], columns=[('feature')])\n",
        "aspect2 = pd.DataFrame(dct[2], columns=[('feature')])\n",
        "aspect3 = pd.DataFrame(dct[3], columns=[('feature')])\n",
        "aspect4 = pd.DataFrame(dct[4], columns=[('feature')])\n",
        "aspect5 = pd.DataFrame(dct[5], columns=[('feature')])\n",
        "aspect6 = pd.DataFrame(dct[6], columns=[('feature')])\n",
        "aspect7 = pd.DataFrame(dct[7], columns=[('feature')])\n",
        "aspect8 = pd.DataFrame(dct[8], columns=[('feature')])\n",
        "aspect9 = pd.DataFrame(dct[9], columns=[('feature')])\n",
        "aspect10 = pd.DataFrame(dct[10], columns=[('feature')])\n",
        "aspect11 = pd.DataFrame(dct[11], columns=[('feature')])\n",
        "aspect12 = pd.DataFrame(dct[12], columns=[('feature')])\n",
        "aspect13 = pd.DataFrame(dct[13], columns=[('feature')])\n",
        "aspect14 = pd.DataFrame(dct[14], columns=[('feature')])\n",
        "aspect15 = pd.DataFrame(dct[15], columns=[('feature')])\n",
        "aspect16 = pd.DataFrame(dct[16], columns=[('feature')])\n",
        "aspect17= pd.DataFrame(dct[17], columns=[('feature')])\n",
        "aspect18 = pd.DataFrame(dct[18], columns=[('feature')])\n",
        "aspect19 = pd.DataFrame(dct[19], columns=[('feature')])\n",
        "aspect20 = pd.DataFrame(dct[20], columns=[('feature')])\n",
        "aspect21 = pd.DataFrame(dct[21], columns=[('feature')])\n",
        "aspect22 = pd.DataFrame(dct[22], columns=[('feature')])\n",
        "aspect23 = pd.DataFrame(dct[23], columns=[('feature')])\n",
        "aspect24 = pd.DataFrame(dct[24], columns=[('feature')])\n",
        "aspect25 = pd.DataFrame(dct[25], columns=[('feature')])\n",
        "aspect26 = pd.DataFrame(dct[26], columns=[('feature')])\n",
        "aspect27 = pd.DataFrame(dct[27], columns=[('feature')])\n",
        "aspect28 = pd.DataFrame(dct[28], columns=[('feature')])\n",
        "aspect29 = pd.DataFrame(dct[29], columns=[('feature')])\n",
        "aspect30 = pd.DataFrame(dct[30], columns=[('feature')])\n",
        "aspect31 = pd.DataFrame(dct[31], columns=[('feature')])\n",
        "aspect32 = pd.DataFrame(dct[32], columns=[('feature')])\n",
        "aspect33 = pd.DataFrame(dct[33], columns=[('feature')])\n",
        "aspect34 = pd.DataFrame(dct[34], columns=[('feature')])\n",
        "aspect35 = pd.DataFrame(dct[35], columns=[('feature')])\n",
        "aspect36 = pd.DataFrame(dct[36], columns=[('feature')])\n",
        "aspect37 = pd.DataFrame(dct[37], columns=[('feature')])\n",
        "aspect38 = pd.DataFrame(dct[38], columns=[('feature')])\n",
        "aspect39 = pd.DataFrame(dct[39], columns=[('feature')])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fMy_pDLcanf"
      },
      "source": [
        "for i in range (len(aspect0['feature'])):\n",
        "  blob = TextBlob(aspect0['feature'][i], analyzer=None,)\n",
        "  dcti[0].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect1['feature'])):\n",
        "  blob = TextBlob(aspect1['feature'][i], analyzer=None,)\n",
        "  dcti[1].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect2['feature'])):\n",
        "  blob = TextBlob(aspect2['feature'][i], analyzer=None,)\n",
        "  dcti[2].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect3['feature'])):\n",
        "  blob = TextBlob(aspect3['feature'][i], analyzer=None,)\n",
        "  dcti[3].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect4['feature'])):\n",
        "  blob = TextBlob(aspect4['feature'][i], analyzer=None,)\n",
        "  dcti[4].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect5['feature'])):\n",
        "  blob = TextBlob(aspect5['feature'][i], analyzer=None,)\n",
        "  dcti[5].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect6['feature'])):\n",
        "  blob = TextBlob(aspect6['feature'][i], analyzer=None,)\n",
        "  dcti[6].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect7['feature'])):\n",
        "  blob = TextBlob(aspect7['feature'][i], analyzer=None,)\n",
        "  dcti[7].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect8['feature'])):\n",
        "  blob = TextBlob(aspect8['feature'][i], analyzer=None,)\n",
        "  dcti[8].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect9['feature'])):\n",
        "  blob = TextBlob(aspect9['feature'][i], analyzer=None,)\n",
        "  dcti[9].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect10['feature'])):\n",
        "  blob = TextBlob(aspect10['feature'][i], analyzer=None,)\n",
        "  dcti[10].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect11['feature'])):\n",
        "  blob = TextBlob(aspect11['feature'][i], analyzer=None,)\n",
        "  dcti[11].append(blob.sentiment)\n",
        "\n",
        "\n",
        "for i in range (len(aspect12['feature'])):\n",
        "  blob = TextBlob(aspect12['feature'][i], analyzer=None,)\n",
        "  dcti[12].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect13['feature'])):\n",
        "  blob = TextBlob(aspect13['feature'][i], analyzer=None,)\n",
        "  dcti[13].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect14['feature'])):\n",
        "  blob = TextBlob(aspect14['feature'][i], analyzer=None,)\n",
        "  dcti[14].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect15['feature'])):\n",
        "  blob = TextBlob(aspect15['feature'][i], analyzer=None,)\n",
        "  dcti[15].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect16['feature'])):\n",
        "  blob = TextBlob(aspect16['feature'][i], analyzer=None,)\n",
        "  dcti[16].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect17['feature'])):\n",
        "  blob = TextBlob(aspect17['feature'][i], analyzer=None,)\n",
        "  dcti[17].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect18['feature'])):\n",
        "  blob = TextBlob(aspect18['feature'][i], analyzer=None,)\n",
        "  dcti[18].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect19['feature'])):\n",
        "  blob = TextBlob(aspect19['feature'][i], analyzer=None,)\n",
        "  dcti[19].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect20['feature'])):\n",
        "  blob = TextBlob(aspect20['feature'][i], analyzer=None,)\n",
        "  dcti[20].append(blob.sentiment)\n",
        "\n",
        "\n",
        "for i in range (len(aspect21['feature'])):\n",
        "  blob = TextBlob(aspect21['feature'][i], analyzer=None,)\n",
        "  dcti[21].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect22['feature'])):\n",
        "  blob = TextBlob(aspect22['feature'][i], analyzer=None,)\n",
        "  dcti[22].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect23['feature'])):\n",
        "  blob = TextBlob(aspect23['feature'][i], analyzer=None,)\n",
        "  dcti[23].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect24['feature'])):\n",
        "  blob = TextBlob(aspect24['feature'][i], analyzer=None,)\n",
        "  dcti[24].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect25['feature'])):\n",
        "  blob = TextBlob(aspect25['feature'][i], analyzer=None,)\n",
        "  dcti[25].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect26['feature'])):\n",
        "  blob = TextBlob(aspect26['feature'][i], analyzer=None,)\n",
        "  dcti[26].append(blob.sentiment)\n",
        "\n",
        "\n",
        "for i in range (len(aspect27['feature'])):\n",
        "  blob = TextBlob(aspect27['feature'][i], analyzer=None,)\n",
        "  dcti[27].append(blob.sentiment)\n",
        "  \n",
        "for i in range (len(aspect28['feature'])):\n",
        "  blob = TextBlob(aspect28['feature'][i], analyzer=None,)\n",
        "  dcti[28].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect29['feature'])):\n",
        "  blob = TextBlob(aspect29['feature'][i], analyzer=None,)\n",
        "  dcti[29].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect30['feature'])):\n",
        "  blob = TextBlob(aspect30['feature'][i], analyzer=None,)\n",
        "  dcti[30].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect31['feature'])):\n",
        "  blob = TextBlob(aspect31['feature'][i], analyzer=None,)\n",
        "  dcti[31].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect32['feature'])):\n",
        "  blob = TextBlob(aspect32['feature'][i], analyzer=None,)\n",
        "  dcti[32].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect33['feature'])):\n",
        "  blob = TextBlob(aspect33['feature'][i], analyzer=None,)\n",
        "  dcti[33].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect34['feature'])):\n",
        "  blob = TextBlob(aspect34['feature'][i], analyzer=None,)\n",
        "  dcti[34].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect35['feature'])):\n",
        "  blob = TextBlob(aspect35['feature'][i], analyzer=None,)\n",
        "  dcti[35].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect36['feature'])):\n",
        "  blob = TextBlob(aspect36['feature'][i], analyzer=None,)\n",
        "  dcti[36].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect37['feature'])):\n",
        "  blob = TextBlob(aspect37['feature'][i], analyzer=None,)\n",
        "  dcti[37].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect38['feature'])):\n",
        "  blob = TextBlob(aspect38['feature'][i], analyzer=None,)\n",
        "  dcti[38].append(blob.sentiment)\n",
        "\n",
        "for i in range (len(aspect39['feature'])):\n",
        "  blob = TextBlob(aspect39['feature'][i], analyzer=None,)\n",
        "  dcti[39].append(blob.sentiment)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvXvQjH3zbfE"
      },
      "source": [
        "size = []\n",
        "for i in range(0,40):\n",
        "  size.append(len(dcti[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64hv4228Dt6u"
      },
      "source": [
        "aspect0  = pd.DataFrame(dcti[0])\n",
        "aspect0 = aspect0.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect0['mean'] = aspect0['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect1  = pd.DataFrame(dcti[1])\n",
        "aspect1 = aspect1.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect1['mean'] = aspect1['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect2  = pd.DataFrame(dcti[2])\n",
        "aspect2 = aspect2.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect2['mean'] = aspect2['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect3  = pd.DataFrame(dcti[3])\n",
        "aspect3 = aspect3.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect3['mean'] = aspect3['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect4  = pd.DataFrame(dcti[4])\n",
        "aspect4 = aspect4.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect4['mean'] = aspect4['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect5  = pd.DataFrame(dcti[5])\n",
        "aspect5 = aspect5.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect5['mean'] = aspect5['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect6  = pd.DataFrame(dcti[6])\n",
        "aspect6 = aspect6.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect6['mean'] = aspect6['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect7  = pd.DataFrame(dcti[7])\n",
        "aspect7 = aspect7.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect7['mean'] = aspect7['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect8  = pd.DataFrame(dcti[8])\n",
        "aspect8 = aspect8.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect8['mean'] = aspect8['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect9  = pd.DataFrame(dcti[9])\n",
        "aspect9 = aspect9.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect9['mean'] = aspect9['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect10  = pd.DataFrame(dcti[10])\n",
        "aspect10 = aspect10.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect10['mean'] = aspect10['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect11  = pd.DataFrame(dcti[11])\n",
        "aspect11 = aspect11.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect11['mean'] = aspect11['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect12  = pd.DataFrame(dcti[12])\n",
        "aspect12 = aspect12.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect12['mean'] = aspect12['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect13  = pd.DataFrame(dcti[13])\n",
        "aspect13 = aspect13.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect13['mean'] = aspect13['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect14  = pd.DataFrame(dcti[14])\n",
        "aspect14 = aspect14.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect14['mean'] = aspect14['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect15  = pd.DataFrame(dcti[15])\n",
        "aspect15 = aspect15.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect15['mean'] = aspect15['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect16  = pd.DataFrame(dcti[16])\n",
        "aspect16 = aspect16.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect16['mean'] = aspect16['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect17  = pd.DataFrame(dcti[17])\n",
        "aspect17 = aspect17.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect17['mean'] = aspect17['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect18  = pd.DataFrame(dcti[18])\n",
        "aspect18 = aspect18.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect18['mean'] = aspect18['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect19  = pd.DataFrame(dcti[19])\n",
        "aspect19 = aspect19.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect19['mean'] = aspect19['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect20 = pd.DataFrame(dcti[20])\n",
        "aspect20 = aspect20.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect20['mean'] = aspect20['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect21  = pd.DataFrame(dcti[21])\n",
        "aspect21 = aspect21.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect21['mean'] = aspect21['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect22  = pd.DataFrame(dcti[22])\n",
        "aspect22 = aspect22.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect22['mean'] = aspect22['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect23  = pd.DataFrame(dcti[23])\n",
        "aspect23 = aspect23.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect23['mean'] = aspect23['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect24  = pd.DataFrame(dcti[24])\n",
        "aspect24 = aspect24.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect24['mean'] = aspect24['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect25  = pd.DataFrame(dcti[25])\n",
        "aspect25 = aspect25.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect25['mean'] = aspect25['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect26  = pd.DataFrame(dcti[26])\n",
        "aspect26 = aspect26.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect26['mean'] = aspect26['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect27  = pd.DataFrame(dcti[27])\n",
        "aspect27 = aspect27.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect27['mean'] = aspect27['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect28  = pd.DataFrame(dcti[28])\n",
        "aspect28 = aspect28.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect28['mean'] = aspect28['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect29  = pd.DataFrame(dcti[29])\n",
        "aspect29 = aspect29.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect29['mean'] = aspect29['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect30  = pd.DataFrame(dcti[30])\n",
        "aspect30 = aspect30.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect30['mean'] = aspect30['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect31  = pd.DataFrame(dcti[31])\n",
        "aspect31 = aspect31.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect31['mean'] = aspect31['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect32  = pd.DataFrame(dcti[32])\n",
        "aspect32 = aspect32.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect32['mean'] = aspect32['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect33  = pd.DataFrame(dcti[33])\n",
        "aspect33 = aspect33.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect33['mean'] = aspect33['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect34  = pd.DataFrame(dcti[34])\n",
        "aspect34 = aspect34.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect34['mean'] = aspect34['polarity'].rolling(100).mean()\n",
        "\n",
        "\n",
        "aspect35 = pd.DataFrame(dcti[35])\n",
        "aspect35 = aspect35.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect35['mean'] = aspect35['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect36 = pd.DataFrame(dcti[36])\n",
        "aspect36 = aspect36.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect36['mean'] = aspect36['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect37  = pd.DataFrame(dcti[37])\n",
        "aspect37 = aspect37.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect37['mean'] = aspect37['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect38  = pd.DataFrame(dcti[38])\n",
        "aspect38 = aspect38.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect38['mean'] = aspect38['polarity'].rolling(100).mean()\n",
        "\n",
        "aspect39  = pd.DataFrame(dcti[39])\n",
        "aspect39 = aspect39.sort_values(by='subjectivity', ascending=False, na_position='first')\n",
        "aspect39['mean'] = aspect39['polarity'].rolling(100).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5bnPqPPs7OS"
      },
      "source": [
        "sentiment = []\n",
        "\n",
        "\n",
        "aspect0 = aspect0.dropna()\n",
        "aspect0 = aspect0.reset_index(drop=True)\n",
        "aspect0['mean'][0]\n",
        "\n",
        "if aspect0['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect1 = aspect1.dropna()\n",
        "aspect1 = aspect1.reset_index(drop=True)\n",
        "aspect1['mean'][0]\n",
        "\n",
        "if aspect1['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect2 = aspect2.dropna()\n",
        "aspect2 = aspect2.reset_index(drop=True)\n",
        "aspect2['mean'][0]\n",
        "\n",
        "if aspect2['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "  \n",
        "\n",
        "aspect3 = aspect3.dropna()\n",
        "aspect3 = aspect3.reset_index(drop=True)\n",
        "aspect3['mean'][0]\n",
        "\n",
        "if aspect3['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect4 = aspect4.dropna()\n",
        "aspect4 = aspect4.reset_index(drop=True)\n",
        "aspect4['mean'][0]\n",
        "\n",
        "if aspect4['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect5 = aspect5.dropna()\n",
        "aspect5 = aspect5.reset_index(drop=True)\n",
        "aspect5['mean'][0]\n",
        "\n",
        "if aspect5['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect6 = aspect6.dropna()\n",
        "aspect6 = aspect6.reset_index(drop=True)\n",
        "aspect6['mean'][0]\n",
        "\n",
        "if aspect6['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect7 = aspect7.dropna()\n",
        "aspect7 = aspect7.reset_index(drop=True)\n",
        "aspect7['mean'][0]\n",
        "\n",
        "\n",
        "if aspect7['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect8 = aspect8.dropna()\n",
        "aspect8 = aspect8.reset_index(drop=True)\n",
        "aspect8['mean'][0]\n",
        "\n",
        "if aspect8['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect9 = aspect9.dropna()\n",
        "aspect9 = aspect9.reset_index(drop=True)\n",
        "aspect9['mean'][0]\n",
        "\n",
        "if aspect9['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect10 = aspect10.dropna()\n",
        "aspect10 = aspect10.reset_index(drop=True)\n",
        "aspect10['mean'][0]\n",
        "\n",
        "\n",
        "if aspect10['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "\n",
        "aspect11 = aspect11.dropna()\n",
        "aspect11 = aspect11.reset_index(drop=True)\n",
        "aspect11['mean'][0]\n",
        "\n",
        "if aspect11['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "\n",
        "aspect12 = aspect12.dropna()\n",
        "aspect12 = aspect12.reset_index(drop=True)\n",
        "aspect12['mean'][0]\n",
        "\n",
        "if aspect12['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect13 = aspect13.dropna()\n",
        "aspect13 = aspect13.reset_index(drop=True)\n",
        "aspect13['mean'][0]\n",
        "\n",
        "\n",
        "if aspect13['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect14 = aspect14.dropna()\n",
        "aspect14 = aspect14.reset_index(drop=True)\n",
        "aspect14['mean'][0]\n",
        "\n",
        "if aspect14['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect15 = aspect15.dropna()\n",
        "aspect15 = aspect15.reset_index(drop=True)\n",
        "aspect15['mean'][0]\n",
        "\n",
        "if aspect15['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect16 = aspect16.dropna()\n",
        "aspect16 = aspect16.reset_index(drop=True)\n",
        "aspect16['mean'][0]\n",
        "\n",
        "if aspect16['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect17 = aspect17.dropna()\n",
        "aspect17 = aspect17.reset_index(drop=True)\n",
        "aspect17['mean'][0]\n",
        "\n",
        "if aspect17['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect18 = aspect18.dropna()\n",
        "aspect18 = aspect18.reset_index(drop=True)\n",
        "aspect18['mean'][0]\n",
        "\n",
        "if aspect18['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect19 = aspect19.dropna()\n",
        "aspect19 = aspect19.reset_index(drop=True)\n",
        "aspect19['mean'][0]\n",
        "\n",
        "if aspect19['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect20 = aspect20.dropna()\n",
        "aspect20 = aspect20.reset_index(drop=True)\n",
        "aspect20['mean'][0]\n",
        "\n",
        "\n",
        "if aspect20['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "  \n",
        "\n",
        "\n",
        "aspect21 = aspect21.dropna()\n",
        "aspect21 = aspect21.reset_index(drop=True)\n",
        "aspect21['mean'][0]\n",
        "\n",
        "if aspect21['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect22 = aspect22.dropna()\n",
        "aspect22 = aspect22.reset_index(drop=True)\n",
        "aspect22['mean'][0]\n",
        "\n",
        "\n",
        "if aspect22['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect23 = aspect23.dropna()\n",
        "aspect23 = aspect23.reset_index(drop=True)\n",
        "aspect23['mean'][0]\n",
        "\n",
        "\n",
        "if aspect23['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect24 = aspect24.dropna()\n",
        "aspect24 = aspect24.reset_index(drop=True)\n",
        "aspect24['mean'][0]\n",
        "\n",
        "if aspect24['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect25 = aspect25.dropna()\n",
        "aspect25 = aspect25.reset_index(drop=True)\n",
        "aspect25['mean'][0]\n",
        "\n",
        "\n",
        "if aspect25['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect26 = aspect26.dropna()\n",
        "aspect26 = aspect26.reset_index(drop=True)\n",
        "aspect26['mean'][0]\n",
        "\n",
        "if aspect26['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect27 = aspect27.dropna()\n",
        "aspect27 = aspect27.reset_index(drop=True)\n",
        "aspect27['mean'][0]\n",
        "\n",
        "if aspect27['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect28 = aspect28.dropna()\n",
        "aspect28 = aspect28.reset_index(drop=True)\n",
        "aspect28['mean'][0]\n",
        "\n",
        "if aspect28['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect29 = aspect29.dropna()\n",
        "aspect29 = aspect29.reset_index(drop=True)\n",
        "aspect29['mean'][0]\n",
        "\n",
        "if aspect29['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect30 = aspect30.dropna()\n",
        "aspect30 = aspect30.reset_index(drop=True)\n",
        "aspect30['mean'][0]\n",
        "\n",
        "if aspect30['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "  \n",
        "\n",
        "aspect31 = aspect31.dropna()\n",
        "aspect31 = aspect31.reset_index(drop=True)\n",
        "aspect31['mean'][0]\n",
        "\n",
        "if aspect31['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect32 = aspect32.dropna()\n",
        "aspect32 = aspect32.reset_index(drop=True)\n",
        "aspect32['mean'][0]\n",
        "\n",
        "\n",
        "if aspect32['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect33 = aspect33.dropna()\n",
        "aspect33 = aspect33.reset_index(drop=True)\n",
        "aspect33['mean'][0]\n",
        "\n",
        "\n",
        "if aspect33['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect34 = aspect34.dropna()\n",
        "aspect34 = aspect34.reset_index(drop=True)\n",
        "aspect34['mean'][0]\n",
        "\n",
        "\n",
        "if aspect34['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect35 = aspect35.dropna()\n",
        "aspect35 = aspect35.reset_index(drop=True)\n",
        "aspect35['mean'][0]\n",
        "\n",
        "\n",
        "if aspect35['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect36 = aspect36.dropna()\n",
        "aspect36 = aspect36.reset_index(drop=True)\n",
        "aspect36['mean'][0]\n",
        "\n",
        "if aspect36['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect37 = aspect37.dropna()\n",
        "aspect37 = aspect37.reset_index(drop=True)\n",
        "aspect37['mean'][0]\n",
        "\n",
        "if aspect37['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "aspect38 = aspect38.dropna()\n",
        "aspect38 = aspect38.reset_index(drop=True)\n",
        "aspect38['mean'][0]\n",
        "\n",
        "\n",
        "if aspect38['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "aspect39 = aspect39.dropna()\n",
        "aspect39 = aspect39.reset_index(drop=True)\n",
        "aspect39['mean'][0]\n",
        "\n",
        "\n",
        "if aspect39['mean'][0] > 0.4:\n",
        "  sentiment.append('positive')\n",
        "else:\n",
        "  sentiment.append('negative')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncFMAq16qtoB"
      },
      "source": [
        "all_aspects['sentiment'] = pd.DataFrame(sentiment)\n",
        "all_aspects['number of review'] = pd.DataFrame(size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHkFaJB1kmLW"
      },
      "source": [
        "all_aspects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLFFEmJM4Lu5"
      },
      "source": [
        "all_aspects['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aqu7SxLVdBa"
      },
      "source": [
        "# # Visualize the topics\n",
        "# pyLDAvis.enable_notebook()\n",
        "# vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "# vis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfmumVOCh5Xl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}