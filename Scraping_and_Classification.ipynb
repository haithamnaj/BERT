{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Scraping and Classification.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOmtdj36e8oN"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syHZbKxfnX5z"
      },
      "source": [
        "!pip install transformers~=3.5.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvW-qKYg47D"
      },
      "source": [
        "pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzwz6b3_f0J3"
      },
      "source": [
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YSzDGjke8od"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYKxQKee7JJ"
      },
      "source": [
        "!pip install language_tool_python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK1R6VRAf5fH"
      },
      "source": [
        "pip install kneed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ETUm0Kte8of"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from kneed import KneeLocator\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PReSO5-oe8og"
      },
      "source": [
        "# %%time\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# import pandas as pd\n",
        "\n",
        "# reviewlist = []\n",
        "\n",
        "# def get_soup(url):\n",
        "#     r = requests.get('http://localhost:8050/render.html', params={'url': url, 'wait': 2})\n",
        "#     soup = BeautifulSoup(r.text, 'html.parser')\n",
        "#     return soup\n",
        "\n",
        "\n",
        "# def get_reviews(soup):\n",
        "#     reviews = soup.find_all('div', {'data-hook': 'review'})\n",
        "#     try:\n",
        "#         for item in reviews:\n",
        "#             review = {\n",
        "#             'product': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
        "#             'title': item.find('a', {'data-hook': 'review-title'}).text.strip(),\n",
        "#             'rating':  float(item.find('i', {'data-hook': 'review-star-rating'}).text.replace('out of 5 stars', '').strip()),\n",
        "#             'body': item.find('span', {'data-hook': 'review-body'}).text.strip(),\n",
        "#             }\n",
        "#             reviewlist.append(review)\n",
        "#     except:\n",
        "#         pass\n",
        "\n",
        "# for x in range(1,500):\n",
        "#     soup = get_soup(f'https://www.amazon.com/Dead-Sea-Mask-Face-Body/product-reviews/B01NCM25K7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews{x}')\n",
        "#     print(f'Getting page: {x}')\n",
        "#     get_reviews(soup)\n",
        "#     print(len(reviewlist))\n",
        "#     if not soup.find('li', {'class': 'a-disabled a-last'}):\n",
        "#         pass\n",
        "#     else:\n",
        "#         break\n",
        "\n",
        "\n",
        "# print('Fin.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkeXqJTye8oh"
      },
      "source": [
        "\n",
        "# df = pd.DataFrame(reviewlist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGRcy3bEe8oi"
      },
      "source": [
        "# df.to_excel('New York Biology Dead Sea Mud Mask for Face and Body.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBiY8ID4e8oi"
      },
      "source": [
        "# #df1 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Sport and outdoors-Kijaro Dual Lock Portable Camping and Sports Chair.xlsx\", index_col=0)  \n",
        "# #df2 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Home and kitchen-Food storage container.xlsx\", index_col=0)  \n",
        "# #df3 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Home and kitchen-Fullstar Vegetable Chopper .xlsx\", index_col=0)  \n",
        "# #df4 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Home and kitchen-GreenLife Soft Grip Healthy Ceramic Nonstick Cookware.xlsx\", index_col=0)  \n",
        "# #df5 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Home and kitchen-Knives.xlsx\", index_col=0)  \n",
        "# df6 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Home and kitchen-Food storage container.xlsx\")  \n",
        "# df7 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\New York Biology Dead Sea Mud Mask for Face and Body.xlsx\")  \n",
        "# #df8 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Sport and outdoors-Outdoor Tactical Bag Backpack.xlsx\", index_col=0)  \n",
        "# df9 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Sport and outdoors-Under Armour Men Locker III Slide Sandal.xlsx\")  \n",
        "# df10 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Osmo - Genius Starter Kit for iPad - 5 Educational Learning Games.xlsx\")  \n",
        "# df11 = pd.read_excel(r\"C:\\Users\\Haitham Najdawi\\Desktop\\Touchless Forehead Thermometer for Adults, Kids and Babies.xlsx\")  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyjRQh3me8ok"
      },
      "source": [
        "# df = df6.append([df7, df9, df10, df11])\n",
        "# df.to_excel('5 categories.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlvYaedfgLjr"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(r\"/content/5 categories.xlsx\",nrows= 500)  \n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3RqJu1er9q"
      },
      "source": [
        "## Check the grammar of the review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3sKMGERewKR"
      },
      "source": [
        "import language_tool_python\n",
        "tool = language_tool_python.LanguageTool('en-US')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4--0kUvewNX"
      },
      "source": [
        "token = []\n",
        "neglected = []\n",
        "size = df['body'].size\n",
        "for i in range (0,len(df['body'])):\n",
        "    if  len(tool.check(df['body'][i])) > 0  :\n",
        "        token.append(i)\n",
        "    else:\n",
        "        neglected.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC9fvkJ4pTZx"
      },
      "source": [
        "len(neglected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtCI5-TSgkrC"
      },
      "source": [
        "for i in(neglected):\n",
        "    df= df.drop(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy2_J0gFewQ5"
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gYNolAewUM"
      },
      "source": [
        "#1. different sentiment in review headline and review body\n",
        "\n",
        "remove_reviews = []\n",
        "# stores the list of review_id of fake reviews\n",
        "\n",
        "for i in range(0,len(df['body'])):\n",
        "    #iterate through the whole dataset\n",
        "    \n",
        "        if(( df[\"title\"][i] ) == ( df[\"body\"][i] ) ):\n",
        "          remove_reviews.append(i)\n",
        "            # checking if the sentiment of the body and the headline are not same\n",
        "            \n",
        "            # remove_reviews.append(dataset[\"review_id\"][i])\n",
        "            # append review_id to the list of fake reviews."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KHgElB2qnsn"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "remove_reviews = []\n",
        "for i in range(len(df['body'])):\n",
        "    #iterate the whole dataset\n",
        "    \n",
        "    words = nltk.word_tokenize(str(df[\"body\"][i]))\n",
        "    #storing the words from the reviews into the list\n",
        "    \n",
        "    tagged_words = nltk.pos_tag(words)\n",
        "    # returns list of tuples of words along with their parts of speech\n",
        "    \n",
        "    nouns_count = 0\n",
        "    verbs_count = 0\n",
        "    \n",
        "    for j in range(len(tagged_words)):\n",
        "        #iterate through all the words\n",
        "\n",
        "        if(tagged_words[j][1].startswith(\"NN\")):\n",
        "            nouns_count+=1\n",
        "            #counts the no. of nouns in the review\n",
        "\n",
        "        if(tagged_words[j][1].startswith(\"VB\")):\n",
        "            verbs_count+=1\n",
        "            #counts the no. of verbs in the review\n",
        "\n",
        "    if(verbs_count>nouns_count):\n",
        "        #comparing the no. of verbs and nouns\n",
        "        remove_reviews.append(i)\n",
        "        #storing the review to be removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP6nyog2rhbI"
      },
      "source": [
        "print(len(remove_reviews))\n",
        "for i in(remove_reviews):\n",
        "    df= df.drop(i)\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGex4mUkW-UM"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtYpv-6VPJg-"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD16NYPPv3e2"
      },
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating <= 2:\n",
        "    return 0\n",
        "  elif rating == 3:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n",
        "df['sentiment'] = df.rating.apply(to_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS3Q2e6Nv3iD"
      },
      "source": [
        "class_names = ['negative', 'neutral', 'positive']\n",
        "ax = sns.countplot(df.sentiment)\n",
        "plt.xlabel('review sentiment')\n",
        "ax.set_xticklabels(class_names);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krtjxcFqv3k2"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikzjkC5Jv3oC"
      },
      "source": [
        "token_lens = []\n",
        "for txt in df.review:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8gX172bxSzD"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk_gwRH3xUp2"
      },
      "source": [
        "MAX_LEN = 150\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxHKOY7vxUx4"
      },
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIxX6xUkxU3m"
      },
      "source": [
        "df_train, df_test = train_test_split(\n",
        "  df,\n",
        "  test_size=0.3,\n",
        "  random_state=RANDOM_SEED\n",
        ")\n",
        "df_val, df_test = train_test_split(\n",
        "  df_test,\n",
        "  test_size=0.5,\n",
        "  random_state=RANDOM_SEED\n",
        ")\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrs3s1LOxU7k"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.review.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwjMshn4VQMp"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQCdaAvvVTKY"
      },
      "source": [
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiHflRRSVVza"
      },
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkuBmxLRyC3m"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4sSAYpmyC6S"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHHhEOT3yC9u"
      },
      "source": [
        "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t89zO6DiybMw"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSmsdxn5ybPp"
      },
      "source": [
        "print(\"h\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMIwLwodybTu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfWwdtMFybWq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVOu0xjzybao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQGvKjq0vXy0"
      },
      "source": [
        "# df['Polarity_Rating'] = df['rating'].apply(lambda x: 'Positive' if x > 3 else('Negative'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAYqfYHdsyv"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb=LabelBinarizer()\n",
        "\n",
        "df['sentiment']=lb.fit_transform(df['Polarity_Rating'])\n",
        "y= df['sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJFFG6gcvbyh"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='rating',data=df, palette='YlGnBu_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egN-ndd4vb9o"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='Polarity_Rating',data=df, palette='summer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkEsAFTOvb__"
      },
      "source": [
        "# df_Positive = df[df['Polarity_Rating'] == 'Positive'][0:7417]\n",
        "# df_Neutral = df[df['Polarity_Rating'] == 'Neutral']\n",
        "# df_Negative = df[df['Polarity_Rating'] == 'Negative']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXQARFFQwB7R"
      },
      "source": [
        "# df_Neutral_over = df_Neutral.sample(5000, replace=True)\n",
        "# df_Negative_over = df_Negative.sample(5000, replace=True)\n",
        "# df = pd.concat([df_Positive, df_Neutral_over, df_Negative_over], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyxtMcfqvcCl"
      },
      "source": [
        "# sns.set_style('whitegrid')\n",
        "# sns.countplot(x='Polarity_Rating',data=df, palette='summer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOyaSSOSe8ok"
      },
      "source": [
        "## Data exploring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdrJPL5We8or"
      },
      "source": [
        "df[\"review\"] = df[\"body\"].str.lower()\n",
        "#df1[\"review\"] = df1[0].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHx0RtEHe8os"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEUkI-yIEA91"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISQJUCxme8os"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "df[\"review\"] = df[\"review\"].apply(lambda text: remove_stopwords(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4L8IldFe8ot"
      },
      "source": [
        "import string\n",
        "import re\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df[\"review\"] = df[\"review\"].apply(lambda text: remove_punctuation(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GMidpzMe8ou"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "df[\"review\"] = df[\"review\"].apply(lambda text: remove_emoji(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-FzA8fpe8ov"
      },
      "source": [
        "def remove_urls(text):\n",
        "  \n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    try:\n",
        "        return url_pattern.sub(r'', text)\n",
        "    except:\n",
        "        print(text)\n",
        "    \n",
        "df[\"review\"] = df[\"review\"].apply(lambda text: remove_urls(text))\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4DOB3YiWYEm"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkGjxQsgwWIs"
      },
      "source": [
        "# df = df[['review', 'Polarity_Rating']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAQ6gjcEjWzl"
      },
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# n_rare_words = 50000\n",
        "# cnt = Counter()\n",
        "\n",
        "# RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "# def remove_rarewords(text):\n",
        "#     \"\"\"custom function to remove the rare words\"\"\"\n",
        "#     return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "# df[\"review\"] = df[\"review\"].apply(lambda text: remove_rarewords(text))\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl-esQ7RIpjk"
      },
      "source": [
        "# from textblob import TextBlob\n",
        "\n",
        "# def get_adjectives(text):\n",
        "#     blob = TextBlob(text)\n",
        "#     return [ word for (word,tag) in blob.tags if tag == \"JJ\" and \"JJR\" and \"JJS\" and \"NN\" and \"NNS\" and \"NNP\" and \"NNPS\" and \"RB\" and \"RBR\"and \"RBS\"]\n",
        "\n",
        "# df['words'] = df['review'].apply(get_adjectives)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa0kANThwb4r"
      },
      "source": [
        "# one_hot = pd.get_dummies(df[\"Polarity_Rating\"])\n",
        "# df.drop(['Polarity_Rating'],axis=1,inplace=True)\n",
        "# df = pd.concat([df,one_hot],axis=1)\n",
        "# len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqfkRJghYLro"
      },
      "source": [
        "X = df['review'].values\n",
        "# y = df.drop('review', axis=1).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIzN7bgvvie2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ruNkGD9vihl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCXfhDr_vilN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4SzH-deYbNe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIBU2_78w8lT"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "embeddings_train = model.encode(X_train, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGZZeoj8xPPT"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "embeddings_test = model.encode(X_test, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euzTalEkWrvc"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "embeddings_train = StandardScaler().fit_transform(embeddings_train) # normalizing the features\n",
        "embeddings_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o11C9dHWr0I"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "embeddings_test = StandardScaler().fit_transform(embeddings_test) # normalizing the features\n",
        "embeddings_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIIIf3d7W-yN"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_b = PCA(n_components=0.95)\n",
        "embeddings_test = pca_b.fit_transform(embeddings_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6MvrtdJW-4U"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_b = PCA(n_components=0.95)\n",
        "embeddings_train = pca_b.fit_transform(embeddings_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yItS2FKFz7ot"
      },
      "source": [
        "print(len(embeddings_train))\n",
        "print(len(embeddings_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0qU3wO6xPR2"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=12673,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=4000,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=500,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "opt=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9297mEkZxPVN"
      },
      "source": [
        "model.fit(x=embeddings_train, y=y_train, batch_size=256, epochs=100, validation_data=(embeddings_test, y_test), verbose=1, callbacks=early_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51lLhMGGw8n8"
      },
      "source": [
        "model_score = model.evaluate(embeddings_test, y_test, batch_size=64, verbose=1)\n",
        "print('Test accuracy:', model_score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-ny0OhsYJqy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzi_E8-DYJu2"
      },
      "source": [
        "a = [\"This is a great movie\"]\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model1 = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "a = model1.encode(a, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzLg1lFvYJzS"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "TextBlob(\"Value for money\").sentiment\n",
        "## Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSSK-ICre8ov"
      },
      "source": [
        "%%time\n",
        "df[\"unigrams\"] = df[\"review\"].apply(nltk.word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvhKJOmoAd5w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcvQStFZe8ow"
      },
      "source": [
        "df['text_length'] = df['unigrams'].apply(len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ye1uJOAe8ox"
      },
      "source": [
        "df['text_length'].value_counts().plot(kind='bar',)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ELYqFIKe8ox"
      },
      "source": [
        "## Tokenize the reviews using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O-1gGQse8oy"
      },
      "source": [
        "# # from nltk import word_tokenize \n",
        "# # df['token'] = [word_tokenize(sent) for sent in df['review']]\n",
        "\n",
        "# df['token'] = [\"[CLS] \" +item + \" [SEP]\" for item in df['review']]\n",
        "# from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# df['token'] = [tokenizer.tokenize(item) for item in df['token']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6HO6DIVe8oy"
      },
      "source": [
        "# token = []\n",
        "# neglected = []\n",
        "# size = df['token'].size\n",
        "# for i in range (0,len(df['token'])):\n",
        "#     if len(df['token'][i]) < 500  :\n",
        "#         token.append(i)\n",
        "#     else:\n",
        "#         neglected.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC45baPle8oz"
      },
      "source": [
        "# df4 =df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vdtFRXae8oz"
      },
      "source": [
        "# for i in(neglected):\n",
        "#     df= df.drop(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0N6Alz7e8o0"
      },
      "source": [
        "# df = df.reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtm4KA4Be8o0"
      },
      "source": [
        "# from nltk.tokenize import sent_tokenize\n",
        "# import torch\n",
        "# from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.metrics import pairwise_distances_argmin_min\n",
        "# import numpy as np\n",
        "\n",
        "# # In[ ]:\n",
        "\n",
        "\n",
        "# def bertSent_embeding(sentences):\n",
        "#     \"\"\"\n",
        "#     Input a list of sentence tokens\n",
        "    \n",
        "#     Output a list of latent vectors, each vector is a sentence representation\n",
        "    \n",
        "#     Note: Bert model produce 12 layers of latent vector, the 'last layer' method is used here,\n",
        "#           other choices includes average last 4 layers, average all layers, etc.\n",
        "    \n",
        "#     \"\"\"\n",
        "#     ## Add sentence head and tail as BERT requested\n",
        "#     marked_sent = [\"[CLS] \" +item + \" [SEP]\" for item in sentences]\n",
        "    \n",
        "#     ## USE Bert tokenizization \n",
        "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     tokenized_sent = [tokenizer.tokenize(item) for item in marked_sent]\n",
        "    \n",
        "#     ## index to BERT vocabulary\n",
        "#     indexed_tokens = [tokenizer.convert_tokens_to_ids(item) for item in tokenized_sent]\n",
        "#     tokens_tensor = [torch.tensor([item]) for item in indexed_tokens]\n",
        "    \n",
        "#     ## add segment id as BERT requested\n",
        "#     segments_ids = [[1] * len(item) for ind,item in enumerate(tokenized_sent)]\n",
        "#     segments_tensors = [torch.tensor([item]) for item in segments_ids]\n",
        "    \n",
        "#     ## load BERT base model and set to evaluation mode\n",
        "#     bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "#     bert_model.eval()\n",
        "    \n",
        "#     ## Output 12 layers of latent vector\n",
        "#     assert len(tokens_tensor) == len(segments_tensors)\n",
        "#     encoded_layers_list = []\n",
        "#     for i in range(len(tokens_tensor)):\n",
        "#         with torch.no_grad():\n",
        "#             encoded_layers, _ = bert_model(tokens_tensor[i], segments_tensors[i])\n",
        "#         encoded_layers_list.append(encoded_layers)\n",
        "    \n",
        "#     ## Use only the last layer vetcor, other choice available\n",
        "#     token_vecs_list = [layers[11][0] for layers in encoded_layers_list]\n",
        "    \n",
        "#     ## Pooling word vector to sentence vector, use mean pooling, other choice available\n",
        "#     sentence_embedding_list = [torch.mean(vec, dim=0).numpy() for vec in token_vecs_list]\n",
        "    \n",
        "    \n",
        "    \n",
        "#     return sentence_embedding_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaQWMWKve8o2"
      },
      "source": [
        "# marked_sent = [\"[CLS] \" +item + \" [SEP]\" for item in df['review']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sC1uWdfe8o2"
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# tokenized_sent = [tokenizer.tokenize(item) for item in marked_sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETOxLBVFe8o3"
      },
      "source": [
        "# segments_ids = [[1] * len(item) for ind,item in enumerate(tokenized_sent)]\n",
        "# segments_tensors = [torch.tensor([item]) for item in segments_ids]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkGx-MR8e8o3"
      },
      "source": [
        "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "# bert_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpzbfcgOe8o4"
      },
      "source": [
        "# indexed_tokens = [tokenizer.convert_tokens_to_ids(item) for item in tokenized_sent]\n",
        "\n",
        "# tokens_tensor = [torch.tensor([item]) for item in indexed_tokens]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU7JqDfae8o4"
      },
      "source": [
        "# assert len(tokens_tensor) == len(segments_tensors)\n",
        "# encoded_layers_list = []\n",
        "# for i in range(len(tokens_tensor)):\n",
        "#     with torch.no_grad():\n",
        "#         encoded_layers, _ = bert_model(tokens_tensor[i], segments_tensors[i])\n",
        "#     encoded_layers_list.append(encoded_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nU1b9Zie8o5"
      },
      "source": [
        "#   ## Use only the last layer vetcor, other choice available\n",
        "# token_vecs_list = [layers[11][0] for layers in encoded_layers_list]\n",
        "# #token_vecs_list = [((layers[-4:][0]).sum(0)) for layers in encoded_layers_list]\n",
        "\n",
        "# #summed_last_4_layers = torch.stack(encoded_layers[-4:]).sum(0)\n",
        "    \n",
        "#     ## Pooling word vector to sentence vector, use mean pooling, other choice available\n",
        "# sentence_embedding_list = [torch.mean(vec, dim=0).numpy() for vec in token_vecs_list]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuO0f9q4e8o5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSUiK5Cie8o5"
      },
      "source": [
        "# %%time\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# def pos(text):\n",
        "#     doc = nlp(text)\n",
        "#     # You want list of Verb tokens \n",
        "#     aspects = [token.text for token in doc if token.pos_ == \"NOUN\" and \"ADJ\"]\n",
        "    \n",
        "#     return aspects\n",
        "\n",
        "\n",
        "# df[\"review\"] = df[\"review\"].apply(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FljcLHYdoT"
      },
      "source": [
        "bert2 = list(df['review'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuliFyJJe8o9"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "embeddings = model.encode(bert2, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKBJhh5Ce8o-"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibUTyWmFe8o-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "x = StandardScaler().fit_transform(embeddings) # normalizing the features\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbXwISGme8o_"
      },
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# x = scaler.fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCmKF4_ue8pA"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA().fit(x)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy9dzasSe8pB"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_b = PCA(n_components=0.95)\n",
        "data_rescaled = pca_b.fit_transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLTdxpGye8pB"
      },
      "source": [
        "data_rescaled.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X0M6iLTnOQo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KirxOsPdnbfa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NIIIoJonbh2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgtA7IM6nbkt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRNUYOJrnboZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aanrIrxDnOYn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQ_vZ2ye8pC"
      },
      "source": [
        "kmeans_kwargs = {\n",
        "    \"init\": \"random\",\n",
        "    \"n_init\": 10,\n",
        "    \"max_iter\": 300,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "# A list holds the SSE values for each k\n",
        "sse = []\n",
        "for k in range(1, 15):\n",
        "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
        "    kmeans.fit(data_rescaled)\n",
        "    sse.append(kmeans.inertia_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rls6mZpe8pC"
      },
      "source": [
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.plot(range(1, 15), sse)\n",
        "plt.xticks(range(1, 15))\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1i6IYTfe8pD"
      },
      "source": [
        "kl = KneeLocator(\n",
        "    range(1, 15), sse, curve=\"convex\", direction=\"decreasing\"\n",
        ")\n",
        "\n",
        "kl.elbow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKL0Eere8pE"
      },
      "source": [
        "#n_clusters = np.ceil(len(projected)**0.5)\n",
        "n_clusters = 5 ##based on elbow-method\n",
        "kmeans = KMeans(n_clusters=int(n_clusters))\n",
        "kmeans = kmeans.fit(data_rescaled)\n",
        "    \n",
        "sum_index,_ = pairwise_distances_argmin_min(kmeans.cluster_centers_, data_rescaled,metric='euclidean')\n",
        "    \n",
        "sum_index = sorted(sum_index)\n",
        "    \n",
        "len(sum_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPUcYX_Je8pF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GCt6-2se8pG"
      },
      "source": [
        "df1 = pd.DataFrame()\n",
        "\n",
        "a = kmeans.predict(data_rescaled)\n",
        "df['cluster'] = pd.DataFrame(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yF_kuSue8pG"
      },
      "source": [
        "df['cluster'] = pd.Series(a, index=df.index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TErZXwzBe8pG"
      },
      "source": [
        "df['cluster'].value_counts().plot(kind='bar')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "absJfA5he8pH"
      },
      "source": [
        "df = df.reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDFmOcske8pH"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJXr4lIqe8pI"
      },
      "source": [
        "num_reviews = df[\"review\"].size\n",
        "\n",
        "reviews_cluster0 = []\n",
        "reviews_cluster1 = []\n",
        "reviews_cluster2 = []\n",
        "reviews_cluster3 = []\n",
        "reviews_cluster4 = []\n",
        "reviews_cluster5 = []\n",
        "\n",
        " \n",
        "for i in range(0, num_reviews):\n",
        "    if df[\"cluster\"][i] == 0:\n",
        "        reviews_cluster0.append(df[\"review\"][i])\n",
        "    \n",
        "\n",
        "    elif df[\"cluster\"][i] == 1:\n",
        "        reviews_cluster1.append(df[\"review\"][i])\n",
        "        \n",
        "\n",
        "    elif df[\"cluster\"][i] == 2:\n",
        "        reviews_cluster2.append(df[\"review\"][i])\n",
        "        \n",
        "\n",
        "    elif df[\"cluster\"][i] == 3:\n",
        "        reviews_cluster3.append(df[\"review\"][i])\n",
        "        \n",
        "        \n",
        "    elif df[\"cluster\"][i] == 4:\n",
        "        reviews_cluster4.append(df[\"review\"][i])\n",
        "       \n",
        "    else:\n",
        "        if df[\"cluster\"][i] == 5:\n",
        "            reviews_cluster5.append(df[\"review\"][i])\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkppw3Sae8pJ"
      },
      "source": [
        "print(len(reviews_cluster0))\n",
        "print(len(reviews_cluster1))\n",
        "print(len(reviews_cluster2))\n",
        "print(len(reviews_cluster3))\n",
        "print(len(reviews_cluster4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHM1okIi1M8t"
      },
      "source": [
        "docs0 = np.array(reviews_cluster0)\n",
        "docs1 = np.array(reviews_cluster1)\n",
        "docs2 = np.array(reviews_cluster2)\n",
        "docs3 = np.array(reviews_cluster3)\n",
        "docs4 = np.array(reviews_cluster4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZTLB92f1M_k"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "word_count_vector0 = cv.fit_transform(docs0).toarray()\n",
        "word_count_vector1 = cv.fit_transform(docs1).toarray()\n",
        "word_count_vector2 = cv.fit_transform(docs2).toarray()\n",
        "word_count_vector3 = cv.fit_transform(docs3).toarray()\n",
        "word_count_vector4 = cv.fit_transform(docs4).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdWh6PgHEW0x"
      },
      "source": [
        "cv0=CountVectorizer(max_df=0.999,stop_words=STOPWORDS,ngram_range=(2,2))\n",
        "cv1=CountVectorizer(max_df=0.999,stop_words=STOPWORDS,ngram_range=(2,2))\n",
        "cv2=CountVectorizer(max_df=0.999,stop_words=STOPWORDS,ngram_range=(2,2))\n",
        "cv3=CountVectorizer(max_df=0.999,stop_words=STOPWORDS,ngram_range=(2,2))\n",
        "cv4=CountVectorizer(max_df=0.999,stop_words=STOPWORDS,ngram_range=(2,2))\n",
        "\n",
        "word_count_vector0=cv0.fit_transform(docs0)\n",
        "word_count_vector1=cv1.fit_transform(docs1)\n",
        "word_count_vector2=cv2.fit_transform(docs2)\n",
        "word_count_vector3=cv3.fit_transform(docs3)\n",
        "word_count_vector4=cv4.fit_transform(docs4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsTj8bp_EW3U"
      },
      "source": [
        "df0 = pd.DataFrame(list(cv0.vocabulary_.keys())[:10])\n",
        "df0 = df0.rename(columns={0: 'Topic0'})\n",
        "df1 = pd.DataFrame(list(cv1.vocabulary_.keys())[:10])\n",
        "df1 = df1.rename(columns={0: 'Topic1'})\n",
        "df2 = pd.DataFrame(list(cv2.vocabulary_.keys())[:10])\n",
        "df2 = df2.rename(columns={0: 'Topic2'})\n",
        "df3 = pd.DataFrame(list(cv3.vocabulary_.keys())[:10])\n",
        "df3 = df3.rename(columns={0: 'Topic3'})\n",
        "df4 = pd.DataFrame(list(cv4.vocabulary_.keys())[:10])\n",
        "df4 = df4.rename(columns={0: 'Topic4'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udZP-w2Kc9by"
      },
      "source": [
        "df_topic = pd.concat([df0,df1 ,df2, df3, df4], axis=1)\n",
        "df_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmYfBfuREW5q"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z64mXNzbEW-7"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqt3qnhfEXCT"
      },
      "source": [
        "# you only needs to do this once\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=docs[0]\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Title=====\")\n",
        "\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhotPl37Fsxr"
      },
      "source": [
        "# put the common code into several methods\n",
        "def get_keywords(idx):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    \n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJRFxORWFs0b"
      },
      "source": [
        "idx=120\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SKcEJA-Fs3J"
      },
      "source": [
        "#generate tf-idf for all documents in your list. docs_test has 500 documents\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform(docs))\n",
        "\n",
        "results=[]\n",
        "for i in range(tf_idf_vector.shape[0]):\n",
        "    \n",
        "    # get vector for a single document\n",
        "    curr_vector=tf_idf_vector[i]\n",
        "    \n",
        "    #sort the tf-idf vector by descending order of scores\n",
        "    sorted_items=sort_coo(curr_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    \n",
        "    results.append(keywords)\n",
        "\n",
        "df0=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
        "df0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTscmiNgFs6q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge8uC71Ye8pS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qda4TWFue8pS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r47wwyM0e8pS"
      },
      "source": [
        "print((top_n_words[2]))\n",
        "df2 = pd.DataFrame(top_n_words[2])\n",
        "df2 = df_1.rename(columns={0: 'Topic2'})\n",
        "df2 = df_1.drop([1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd8GTfkWe8pT"
      },
      "source": [
        "print((top_n_words[3]))\n",
        "df2 = pd.DataFrame(top_n_words[3])\n",
        "df2 = df2.rename(columns={0: 'Topic3'})\n",
        "df2 = df2.drop([1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beLH89Yye8pT"
      },
      "source": [
        "print((top_n_words[4]))\n",
        "df2 = pd.DataFrame(top_n_words[4])\n",
        "df2 = df2.rename(columns={0: 'Topic4'})\n",
        "df2 = df2.drop([1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUYuo9u3e8pU"
      },
      "source": [
        "df_topic = pd.concat([df1,df0 ,df_1, df2], axis=1)\n",
        "df_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9XRXigPe8pU"
      },
      "source": [
        "df_topic = pd.concat([df1, df2,df0 ,df_1], axis=1)\n",
        "df_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83PuPd7Ce8pU"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "\n",
        "def twenty_newsgroup_to_csv():\n",
        "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
        "    df.columns = ['text', 'target']\n",
        "\n",
        "    targets = pd.DataFrame( newsgroups_train.target_names)\n",
        "    targets.columns=['title']\n",
        "\n",
        "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
        "    out['date'] = pd.to_datetime('now')\n",
        "    out.to_csv('20_newsgroup.csv')\n",
        "    \n",
        "twenty_newsgroup_to_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwilbLx9e8pV"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "data = fetch_20newsgroups(subset='all')['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJCqZoXPe8pV"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtm5Amrle8pW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nx-K-GVe8pW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WViHNb6ve8pW"
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPYgL0g2e8pX"
      },
      "source": [
        "def getAmazonSearch(search_query):\n",
        "    url=\"https://www.amazon.in/s?k=\"+search_query\n",
        "    print(url)\n",
        "    page=requests.get(url,cookies=cookie,headers=header)\n",
        "    if page.status_code==200:\n",
        "        return page\n",
        "    else:\n",
        "        return \"Error\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbX-BKLKe8pX"
      },
      "source": [
        "data_asin=[]\n",
        "cookie={}\n",
        "response=getAmazonSearch('titan+men+watches')\n",
        "soup=BeautifulSoup(response.content)\n",
        "for i in soup.findAll(\"div\",{'class':\"sg-col-4-of-24 sg-col-4-of-12 sg-col-4-of-36 s-result-item sg-col-4-of-28 sg-col-4-of-16 sg-col sg-col-4-of-20 sg-col-4-of-32\"}):\n",
        "    data_asin.append(i['data-asin'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgKtzlhge8pX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}